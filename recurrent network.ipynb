{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to save hidden layer\n",
    "import copy\n",
    "#matrix math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input data - binary numbers for each integer from 0 to 256\n",
    "int_to_binary = {}\n",
    "binary_dim = 8\n",
    "max_val = (2**binary_dim) #2^8 = 256\n",
    "binary_val = np.unpackbits(np.array([range(max_val)], dtype=np.uint8).T, axis=1) # Calc Binary values for ints 0-256\n",
    "for i in range(max_val): # map Integer values to Binary values\n",
    "    int_to_binary[i] = binary_val[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def activate(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[ 4.07553155]\n",
      "Pred:[0 0 1 1 1 1 0 0]\n",
      "True:[0 1 1 0 0 0 1 1]\n",
      "62 + 37 = 60\n",
      "------------\n",
      "Error:[ 1.7905746]\n",
      "Pred:[1 0 1 0 1 0 0 1]\n",
      "True:[1 0 1 0 1 0 0 1]\n",
      "67 + 102 = 169\n",
      "------------\n",
      "Error:[ 0.67219956]\n",
      "Pred:[1 0 1 0 0 1 0 0]\n",
      "True:[1 0 1 0 0 1 0 0]\n",
      "39 + 125 = 164\n",
      "------------\n",
      "Error:[ 0.27309937]\n",
      "Pred:[1 0 1 0 1 0 1 0]\n",
      "True:[1 0 1 0 1 0 1 0]\n",
      "83 + 87 = 170\n",
      "------------\n",
      "Error:[ 0.09328066]\n",
      "Pred:[0 0 0 1 1 1 1 1]\n",
      "True:[0 0 0 1 1 1 1 1]\n",
      "22 + 9 = 31\n",
      "------------\n",
      "Error:[ 0.14276206]\n",
      "Pred:[0 1 0 0 0 0 1 1]\n",
      "True:[0 1 0 0 0 0 1 1]\n",
      "59 + 8 = 67\n",
      "------------\n",
      "Error:[ 0.14549186]\n",
      "Pred:[1 1 0 1 1 0 1 1]\n",
      "True:[1 1 0 1 1 0 1 1]\n",
      "109 + 110 = 219\n",
      "------------\n",
      "Error:[ 0.09007097]\n",
      "Pred:[0 1 0 1 1 0 1 0]\n",
      "True:[0 1 0 1 1 0 1 0]\n",
      "54 + 36 = 90\n",
      "------------\n",
      "Error:[ 0.1074296]\n",
      "Pred:[1 1 0 1 0 0 1 0]\n",
      "True:[1 1 0 1 0 0 1 0]\n",
      "89 + 121 = 210\n",
      "------------\n",
      "Error:[ 0.09466737]\n",
      "Pred:[1 0 0 0 1 1 1 0]\n",
      "True:[1 0 0 0 1 1 1 0]\n",
      "85 + 57 = 142\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "inputLayerSize = 2\n",
    "hiddenLayerSize = 16\n",
    "outputLayerSize = 1\n",
    "\n",
    "# 3 weight values\n",
    "W1 = 2 * np.random.random((inputLayerSize, hiddenLayerSize)) - 1\n",
    "W2 = 2 * np.random.random((hiddenLayerSize, outputLayerSize)) - 1\n",
    "W_h = 2 * np.random.random((hiddenLayerSize, hiddenLayerSize)) - 1 #Current h to h in next Timestep, recurrence!\n",
    "\n",
    "# Initialize Updated Weights Values\n",
    "W1_update = np.zeros_like(W1)\n",
    "W2_update = np.zeros_like(W2)\n",
    "W_h_update = np.zeros_like(W_h)\n",
    "\n",
    "#Compute the the Sum of two integers \n",
    "for j in range(10000):\n",
    "    \n",
    "    #a + b = c (random values)\n",
    "    a_int = np.random.randint(max_val/2)\n",
    "    b_int = np.random.randint(max_val/2)\n",
    "    c_int = a_int + b_int\n",
    "    \n",
    "    # get binary values for a,b, and c\n",
    "    a = int_to_binary[a_int]\n",
    "    b = int_to_binary[b_int]\n",
    "    c = int_to_binary[c_int]\n",
    "\n",
    "    # Save predicted binary outputs \n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    #Initialize Error\n",
    "    overallError = 0\n",
    "\n",
    "    # Store output gradients & hidden layer values\n",
    "    output_layer_gradients = list()\n",
    "    hidden_layer_values = list()\n",
    "    hidden_layer_values.append(np.zeros(hiddenLayerSize))#init as 0\n",
    "\n",
    "    # Forward propagation to compute the sum of two 8 digit long binary integers\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        #input - binary values of a & b\n",
    "        X = np.array([[a[binary_dim - position - 1], b[binary_dim - position - 1]]])\n",
    "        #output - the sum c\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # Calculate the error\n",
    "        layer_1 = activate(np.dot(X,W1) + np.dot(hidden_layer_values[-1],W_h))\n",
    "        layer_2 = activate(np.dot(layer_1, W2))\n",
    "        output_error = y - layer_2\n",
    "\n",
    "        # Save the error gradients at each step as it will be propagated back\n",
    "        output_layer_gradients.append((output_error)*activate(layer_2, deriv=True))\n",
    "\n",
    "        # Save the sum of error at each binary position\n",
    "        overallError += np.abs(output_error[0])\n",
    "\n",
    "        # Round off the values to nearest \"0\" or \"1\" and save it to a list\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "\n",
    "        # Save the hidden layer to be used later\n",
    "        hidden_layer_values.append(copy.deepcopy(layer_1))\n",
    "\n",
    "    future_layer_1_gradient = np.zeros(hiddenLayerSize)\n",
    "\n",
    "    #backpropagate the error to the previous timesteps!\n",
    "    for position in range(binary_dim):\n",
    "        # a[0], b[0] -> a[1]b[1] ....\n",
    "        X = np.array([[a[position], b[position]]])\n",
    "        # The last step Hidden Layer where we are currently a[0],b[0]\n",
    "        layer_1 = hidden_layer_values[-position - 1]\n",
    "        # The hidden layer before the current layer, a[1],b[1]\n",
    "        prev_hidden_layer = hidden_layer_values[-position-2]\n",
    "        # Errors at Output Layer, a[1],b[1]\n",
    "        output_layer_gradient = output_layer_gradients[-position-1]\n",
    "        layer_1_gradients = (future_layer_1_gradient.dot(W_h.T) + output_layer_gradient.dot(W2.T)) * activate(layer_1, deriv=True)\n",
    "\n",
    "        # Update all the weights and try again\n",
    "        W2_update += np.atleast_2d(layer_1).T.dot(output_layer_gradient)\n",
    "        W_h_update += np.atleast_2d(prev_hidden_layer).T.dot(layer_1_gradients)\n",
    "        W1_update += X.T.dot(layer_1_gradients)\n",
    "\n",
    "        future_layer_1_gradient = layer_1_gradients\n",
    "\n",
    "    # Update the weights with the values\n",
    "    W1 += W1_update \n",
    "    W2 += W2_update\n",
    "    W_h += W_h_update \n",
    "\n",
    "    # Clear the updated weights values\n",
    "    W1_update *= 0\n",
    "    W2_update *= 0\n",
    "    W_h_update *= 0\n",
    "    \n",
    "    # Print out the Progress of the RNN\n",
    "    if (j % 1000 == 0):\n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index, x in enumerate(reversed(d)):\n",
    "            out += x * pow(2, index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
